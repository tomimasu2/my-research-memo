@article{athey2016jul,
  title = {Recursive {{Partitioning}} for {{Heterogeneous Causal Effects}}},
  author = {Athey, Susan and Imbens, Guido},
  year = 2016,
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {27},
  eprint = {1504.01132},
  primaryclass = {stat},
  pages = {7353--7360},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1510489113},
  urldate = {2026-01-25},
  abstract = {In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/4FFE6T3I/Athey と Imbens - 2016 - Recursive Partitioning for Heterogeneous Causal Effects.pdf}
}

@misc{byambadalai2025oct,
  title = {Beyond the {{Average}}: {{Distributional Causal Inference}} under {{Imperfect Compliance}}},
  shorttitle = {Beyond the {{Average}}},
  author = {Byambadalai, Undral and Hirata, Tomu and Oka, Tatsushi and Yasui, Shota},
  year = 2025,
  month = oct,
  number = {arXiv:2509.15594},
  eprint = {2509.15594},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.15594},
  urldate = {2026-01-26},
  abstract = {We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect-the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/yutatomimasu/Zotero/storage/UWXQ9HMA/Byambadalai et al. - 2025 - Beyond the Average Distributional Causal Inference under Imperfect Compliance.pdf}
}

@misc{carro2024dec,
  title = {Do {{Large Language Models Show Biases}} in {{Causal Learning}}?},
  author = {Carro, Maria Victoria and Selasco, Francisca Gauna and Mester, Denise Alejandra and Gonzales, Margarita and Leiva, Mario A. and Martinez, Maria Vanina and Simari, Gerardo I.},
  year = 2024,
  month = dec,
  number = {arXiv:2412.10509},
  eprint = {2412.10509},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.10509},
  urldate = {2026-02-01},
  abstract = {Causal learning is the cognitive process of developing the capability of making causal inferences based on available information, often guided by normative principles. This process is prone to errors and biases, such as the illusion of causality, in which people perceive a causal relationship between two variables despite lacking supporting evidence. This cognitive bias has been proposed to underlie many societal problems, including social prejudice, stereotype formation, misinformation, and superstitious thinking. In this research, we investigate whether large language models (LLMs) develop causal illusions, both in real-world and controlled laboratory contexts of causal learning and inference. To this end, we built a dataset of over 2K samples including purely correlational cases, situations with null contingency, and cases where temporal information excludes the possibility of causality by placing the potential effect before the cause. We then prompted the models to make statements or answer causal questions to evaluate their tendencies to infer causation erroneously in these structured settings. Our findings show a strong presence of causal illusion bias in LLMs. Specifically, in open-ended generation tasks involving spurious correlations, the models displayed bias at levels comparable to, or even lower than, those observed in similar studies on human subjects. However, when faced with null-contingency scenarios or temporal cues that negate causal relationships, where it was required to respond on a 0-100 scale, the models exhibited significantly higher bias. These findings suggest that the models have not uniformly, consistently, or reliably internalized the normative principles essential for accurate causal learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/YHYURNQZ/Carro et al. - 2024 - Do Large Language Models Show Biases in Causal Learning.pdf}
}

@article{chen2022jun,
  title = {Imbalance-{{Aware Uplift Modeling}} for {{Observational Data}}},
  author = {Chen, Xuanying and Liu, Zhining and Yu, Li and Yao, Liuyi and Zhang, Wenpeng and Dong, Yi and Gu, Lihong and Zeng, Xiaodong and Tan, Yize and Gu, Jinjie},
  year = 2022,
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6313--6321},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i6.20581},
  urldate = {2026-01-26},
  abstract = {Uplift modeling aims to model the incremental impact of a treatment on an individual outcome, which has attracted great interests of researchers and practitioners from different communities. Existing uplift modeling methods rely on either the data collected from randomized controlled trials (RCTs) or the observational data which is more realistic. However, we notice that on the observational data, it is often the case that only a small number of subjects receive treatment, but finally infer the uplift on a much large group of subjects. Such highly imbalanced data is common in various fields such as marketing and medical treatment but it is rarely handled by existing works. In this paper, we theoretically and quantitatively prove that the existing representative methods, transformed outcome (TOM) and doubly robust (DR), suffer from large bias and deviation on highly imbalanced datasets with skewed propensity scores, mainly because they are proportional to the reciprocal of the propensity score. To reduce the bias and deviation of uplift modeling with an imbalanced dataset, we propose an imbalance-aware uplift modeling (IAUM) method via constructing a robust proxy outcome, which adaptively combines the doubly robust estimator and the imputed treatment effects based on the propensity score. We theoretically prove that IAUM can obtain a better bias-variance trade-off than existing methods on a highly imbalanced dataset. We conduct extensive experiments on a synthetic dataset and two real-world datasets, and the experimental results well demonstrate the superiority of our method over state-of-the-art.},
  file = {/Users/yutatomimasu/Zotero/storage/CVAYBVCS/Chen et al. - 2022 - Imbalance-Aware Uplift Modeling for Observational Data.pdf}
}

@misc{chernozhukov2024nov,
  title = {Double/{{Debiased Machine Learning}} for {{Treatment}} and {{Causal Parameters}}},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = 2024,
  month = nov,
  number = {arXiv:1608.00060},
  eprint = {1608.00060},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1608.00060},
  urldate = {2026-01-28},
  abstract = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/RQLWSHJ3/Chernozhukov et al. - 2024 - DoubleDebiased Machine Learning for Treatment and Causal Parameters.pdf}
}

@incollection{gao2025,
  title = {{{UTBoost}}: {{Gradient Boosted Decision Trees}} for {{Uplift Modeling}}},
  shorttitle = {{{UTBoost}}},
  author = {Gao, Junjie and Zheng, Xiangyu and Wang, DongDong and Huang, Zhixiang and Zheng, Bangqi and Yang, Kai},
  year = 2025,
  volume = {15281},
  eprint = {2312.02573},
  primaryclass = {cs},
  pages = {41--53},
  doi = {10.1007/978-981-96-0116-5_4},
  urldate = {2026-01-26},
  abstract = {Uplift modeling comprises a collection of machine learning techniques designed for managers to predict the incremental impact of specific actions on customer outcomes. However, accurately estimating this incremental impact poses significant challenges due to the necessity of determining the difference between two mutually exclusive outcomes for each individual. In our study, we introduce two novel modifications to the established Gradient Boosting Decision Trees (GBDT) technique. These modifications sequentially learn the causal effect, addressing the counterfactual dilemma. Each modification innovates upon the existing technique in terms of the ensemble learning method and the learning objective, respectively. Experiments with large-scale datasets validate the effectiveness of our methods, consistently achieving substantial improvements over baseline models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/8KWR6LC5/Gao et al. - 2025 - UTBoost Gradient Boosted Decision Trees for Uplift Modeling.pdf}
}

@misc{hahn2019nov,
  title = {Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects},
  shorttitle = {Bayesian Regression Tree Models for Causal Inference},
  author = {Hahn, P. Richard and Murray, Jared S. and Carvalho, Carlos},
  year = 2019,
  month = nov,
  number = {arXiv:1706.09523},
  eprint = {1706.09523},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.09523},
  urldate = {2026-01-28},
  abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects from observational data, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively "shrink to homogeneity". We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/yutatomimasu/Zotero/storage/7E4H4IF2/Hahn et al. - 2019 - Bayesian regression tree models for causal inference regularization, confounding, and heterogeneous.pdf}
}

@article{hu2024jul,
  title = {Uplift Modeling with Quasi-Loss-Functions},
  author = {Hu, Jinping and De Haan, Evert and Skiera, Bernd},
  year = 2024,
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {38},
  number = {4},
  pages = {2495--2519},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-024-01042-x},
  urldate = {2026-02-01},
  langid = {english},
  file = {/Users/yutatomimasu/Zotero/storage/DZNRWWC5/Hu et al. - 2024 - Uplift modeling with quasi-loss-functions.pdf}
}

@misc{huang2024feb,
  title = {Entire {{Chain Uplift Modeling}} with {{Context-Enhanced Learning}} for {{Intelligent Marketing}}},
  author = {Huang, Yinqiu and Wang, Shuli and Gao, Min and Wei, Xue and Li, Changhao and Luo, Chuan and Zhu, Yinhua and Xiao, Xiong and Luo, Yi},
  year = 2024,
  month = feb,
  number = {arXiv:2402.03379},
  eprint = {2402.03379},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03379},
  urldate = {2026-01-26},
  abstract = {Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1) the Entire Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE throughout the entire chain space, models the various impacts of treatments on each task, and integrates task prior information to enhance context awareness across all stages, capturing the impact of treatment on different tasks, and 2) the Treatment-Enhanced Network, which facilitates fine-grained treatment modeling through bit-level feature interactions, thereby enabling adaptive feature adjustment. Extensive experiments on public and industrial datasets validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan food delivery platform, serving millions of daily active users, with the related dataset released for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/C8P8TF6J/Huang et al. - 2024 - Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing.pdf}
}

@article{imai2013mar,
  title = {Estimating Treatment Effect Heterogeneity in Randomized Program Evaluation},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = 2013,
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {7},
  number = {1},
  eprint = {1305.5682},
  primaryclass = {stat},
  issn = {1932-6157},
  doi = {10.1214/12-AOAS593},
  urldate = {2026-01-28},
  abstract = {When evaluating the efficacy of social programs and medical treatments using randomized experiments, the estimated overall average causal effect alone is often of limited value and the researchers must investigate when the treatments do and do not work. Indeed, the estimation of treatment effect heterogeneity plays an essential role in (1) selecting the most effective treatment from a large number of available treatments, (2) ascertaining subpopulations for which a treatment is effective or harmful, (3) designing individualized optimal treatment regimes, (4) testing for the existence or lack of heterogeneous treatment effects, and (5) generalizing causal effect estimates obtained from an experimental sample to a target population. In this paper, we formulate the estimation of heterogeneous treatment effects as a variable selection problem. We propose a method that adapts the Support Vector Machine classifier by placing separate sparsity constraints over the pre-treatment parameters and causal heterogeneity parameters of interest. The proposed method is motivated by and applied to two well-known randomized evaluation studies in the social sciences. Our method selects the most effective voter mobilization strategies from a large number of alternative strategies, and it also identifies the characteristics of workers who greatly benefit from (or are negatively affected by) a job training program. In our simulation studies, we find that the proposed method often outperforms some commonly used alternatives.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/yutatomimasu/Zotero/storage/3BE4D7NN/Imai と Ratkovic - 2013 - Estimating treatment effect heterogeneity in randomized program evaluation.pdf}
}

@misc{laan2024feb,
  title = {Combining {{T-learning}} and {{DR-learning}}: A Framework for Oracle-Efficient Estimation of Causal Contrasts},
  shorttitle = {Combining {{T-learning}} and {{DR-learning}}},
  author = {van der Laan, Lars and Carone, Marco and Luedtke, Alex},
  year = 2024,
  month = feb,
  number = {arXiv:2402.01972},
  eprint = {2402.01972},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01972},
  urldate = {2026-01-26},
  abstract = {We introduce efficient plug-in (EP) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. The EP-learning framework enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies, such as DR-learning and R-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. To avoid these drawbacks, EP-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like T-learning. Under reasonable conditions, EP-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalence to the minimizer of an oracle-efficient one-step debiased estimator of the population risk function. In simulation experiments, we illustrate that EP-learners of the conditional average treatment effect and conditional relative risk outperform state-of-the-art competitors, including T-learner, R-learner, and DR-learner. Open-source implementations of the proposed methods are available in our R package hte3.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/yutatomimasu/Zotero/storage/LAKBVJFL/Laan et al. - 2024 - Combining T-learning and DR-learning a framework for oracle-efficient estimation of causal contrast.pdf}
}

@inproceedings{li2024may,
  title = {A {{New Transformation Approach}} for {{Uplift Modeling}} with {{Binary Outcome}}},
  booktitle = {Proceedings of the 2024 9th {{International Conference}} on {{Mathematics}} and {{Artificial Intelligence}}},
  author = {Li, Kun and Zhu, Liangshu},
  year = 2024,
  month = may,
  eprint = {2310.05549},
  primaryclass = {stat},
  pages = {73--77},
  doi = {10.1145/3670085.3670089},
  urldate = {2026-01-26},
  abstract = {Uplift modeling has been used effectively in fields such as marketing and customer retention, to target those customers who are more likely to respond due to the campaign or treatment. Essentially, it is a machine learning technique that predicts the gain from performing some action with respect to not taking it. A popular class of uplift models is the transformation approach that redefines the target variable with the original treatment indicator. These transformation approaches only need to train and predict the difference in outcomes directly. The main drawback of these approaches is that in general it does not use the information in the treatment indicator beyond the construction of the transformed outcome and usually is not efficient. In this paper, we design a novel transformed outcome for the case of the binary target variable and unlock the full value of the samples with zero outcome. From a practical perspective, our new approach is flexible and easy to use. Experimental results on synthetic and real-world datasets obviously show that our new approach outperforms the traditional one. At present, our new approach has already been applied to precision marketing in a China nation-wide financial holdings group.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/BMIQQBSX/Li と Zhu - 2024 - A New Transformation Approach for Uplift Modeling with Binary Outcome.pdf}
}

@misc{nie2020aug,
  title = {Quasi-{{Oracle Estimation}} of {{Heterogeneous Treatment Effects}}},
  author = {Nie, Xinkun and Wager, Stefan},
  year = 2020,
  month = aug,
  number = {arXiv:1712.04912},
  eprint = {1712.04912},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04912},
  urldate = {2026-01-28},
  abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/4Z8CJGF9/Nie と Wager - 2020 - Quasi-Oracle Estimation of Heterogeneous Treatment Effects.pdf}
}

@misc{oprescu2019sep,
  title = {Orthogonal {{Random Forest}} for {{Causal Inference}}},
  author = {Oprescu, Miruna and Syrgkanis, Vasilis and Wu, Zhiwei Steven},
  year = 2019,
  month = sep,
  number = {arXiv:1806.03467},
  eprint = {1806.03467},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.03467},
  urldate = {2026-01-26},
  abstract = {We propose the orthogonal random forest, an algorithm that combines Neyman-orthogonality to reduce sensitivity with respect to estimation error of nuisance parameters with generalized random forests (Athey et al., 2017)--a flexible non-parametric method for statistical estimation of conditional moment models using random forests. We provide a consistency rate and establish asymptotic normality for our estimator. We show that under mild assumptions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a priori knowledge of these nuisance parameters. We show that when the nuisance functions have a locally sparse parametrization, then a local \$\textbackslash ell\_1\$-penalized regression achieves the required rate. We apply our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike prior work, our method provably allows to control for a high-dimensional set of variables under standard sparsity conditions. We also provide a comprehensive empirical evaluation of our algorithm on both synthetic and real data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/VITNGE5W/Oprescu et al. - 2019 - Orthogonal Random Forest for Causal Inference.pdf}
}

@misc{saito2022jun,
  title = {Off-{{Policy Evaluation}} for {{Large Action Spaces}} via {{Embeddings}}},
  author = {Saito, Yuta and Joachims, Thorsten},
  year = 2022,
  month = jun,
  number = {arXiv:2202.06317},
  eprint = {2202.06317},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.06317},
  urldate = {2026-01-26},
  abstract = {Off-policy evaluation (OPE) in contextual bandits has seen rapid adoption in real-world systems, since it enables offline evaluation of new policies using only historic log data. Unfortunately, when the number of actions is large, existing OPE estimators -- most of which are based on inverse propensity score weighting -- degrade severely and can suffer from extreme bias and variance. This foils the use of OPE in many applications from recommender systems to language models. To overcome this issue, we propose a new OPE estimator that leverages marginalized importance weights when action embeddings provide structure in the action space. We characterize the bias, variance, and mean squared error of the proposed estimator and analyze the conditions under which the action embedding provides statistical benefits over conventional estimators. In addition to the theoretical analysis, we find that the empirical performance improvement can be substantial, enabling reliable OPE even when existing estimators collapse due to a large number of actions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/DYF8PAIN/Saito と Joachims - 2022 - Off-Policy Evaluation for Large Action Spaces via Embeddings.pdf}
}

@article{temporary-citekey-37,
  title = {The Central Role of the Propensity Score in Observational Studies for Causal Effects},
  author = {Rosenbaum, Paul R and Rubin, Donald B},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
  langid = {english},
  file = {/Users/yutatomimasu/Zotero/storage/ZK8RMKNG/Rosenbaum と Rubin - The central role of the propensity score in observational studies for causal effects.pdf}
}

@misc{udagawa2023jan,
  title = {Policy-{{Adaptive Estimator Selection}} for {{Off-Policy Evaluation}}},
  author = {Udagawa, Takuma and Kiyohara, Haruka and Narita, Yusuke and Saito, Yuta and Tateno, Kei},
  year = 2023,
  month = jan,
  number = {arXiv:2211.13904},
  eprint = {2211.13904},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.13904},
  urldate = {2026-01-26},
  abstract = {Off-policy evaluation (OPE) aims to accurately evaluate the performance of counterfactual policies using only offline logged data. Although many estimators have been developed, there is no single estimator that dominates the others, because the estimators' accuracy can vary greatly depending on a given OPE task such as the evaluation policy, number of actions, and noise level. Thus, the data-driven estimator selection problem is becoming increasingly important and can have a significant impact on the accuracy of OPE. However, identifying the most accurate estimator using only the logged data is quite challenging because the ground-truth estimation accuracy of estimators is generally unavailable. This paper studies this challenging problem of estimator selection for OPE for the first time. In particular, we enable an estimator selection that is adaptive to a given OPE task, by appropriately subsampling available logged data and constructing pseudo policies useful for the underlying estimator selection task. Comprehensive experiments on both synthetic and real-world company data demonstrate that the proposed procedure substantially improves the estimator selection compared to a non-adaptive heuristic.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/TAJ2VJYJ/Udagawa et al. - 2023 - Policy-Adaptive Estimator Selection for Off-Policy Evaluation.pdf}
}

@misc{vansteelandt2023nov,
  title = {Orthogonal Prediction of Counterfactual Outcomes},
  author = {Vansteelandt, Stijn and Morzywo{\l}ek, Pawe{\l}},
  year = 2023,
  month = nov,
  number = {arXiv:2311.09423},
  eprint = {2311.09423},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09423},
  urldate = {2026-01-26},
  abstract = {Orthogonal meta-learners, such as DR-learner, R-learner and IF-learner, are increasingly used to estimate conditional average treatment effects. They improve convergence rates relative to na\"ive meta-learners (e.g., T-, S- and X-learner) through de-biasing procedures that involve applying standard learners to specifically transformed outcome data. This leads them to disregard the possibly constrained outcome space, which can be particularly problematic for dichotomous outcomes: these typically get transformed to values that are no longer constrained to the unit interval, making it difficult for standard learners to guarantee predictions within the unit interval. To address this, we construct orthogonal meta-learners for the prediction of counterfactual outcomes which respect the outcome space. As such, the obtained i-learner or imputation-learner is more generally expected to outperform existing learners, even when the outcome is unconstrained, as we confirm empirically in simulation studies and an analysis of critical care data. Our development also sheds broader light onto the construction of orthogonal learners for other estimands.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/yutatomimasu/Zotero/storage/TDMBXM8V/Vansteelandt と Morzywołek - 2023 - Orthogonal prediction of counterfactual outcomes.pdf}
}

@misc{verhelst2023dec,
  title = {A Churn Prediction Dataset from the Telecom Sector: A New Benchmark for Uplift Modeling},
  shorttitle = {A Churn Prediction Dataset from the Telecom Sector},
  author = {Verhelst, Th{\'e}o and Mercier, Denis and Shrestha, Jeevan and Bontempi, Gianluca},
  year = 2023,
  month = dec,
  number = {arXiv:2312.07206},
  eprint = {2312.07206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.07206},
  urldate = {2026-01-26},
  abstract = {Uplift modeling, also known as individual treatment effect (ITE) estimation, is an important approach for data-driven decision making that aims to identify the causal impact of an intervention on individuals. This paper introduces a new benchmark dataset for uplift modeling focused on churn prediction, coming from a telecom company in Belgium, Orange Belgium. Churn, in this context, refers to customers terminating their subscription to the telecom service. This is the first publicly available dataset offering the possibility to evaluate the efficiency of uplift modeling on the churn prediction problem. Moreover, its unique characteristics make it more challenging than the few other public uplift datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yutatomimasu/Zotero/storage/CGAGZ9RA/Verhelst et al. - 2023 - A churn prediction dataset from the telecom sector a new benchmark for uplift modeling.pdf}
}

@misc{zhu2022feb,
  title = {Addressing {{Positivity Violations}} in {{Causal Effect Estimation}} Using {{Gaussian Process Priors}}},
  author = {Zhu, Yaqian and Mitra, Nandita and Roy, Jason},
  year = 2022,
  month = feb,
  number = {arXiv:2110.10266},
  eprint = {2110.10266},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.10266},
  urldate = {2026-01-26},
  abstract = {In observational studies, causal inference relies on several key identifying assumptions. One identifiability condition is the positivity assumption, which requires the probability of treatment be bounded away from 0 and 1. That is, for every covariate combination, it should be possible to observe both treated and control subjects, i.e., the covariate distributions should overlap between treatment arms. If the positivity assumption is violated, population-level causal inference necessarily involves some extrapolation. Ideally, a greater amount of uncertainty about the causal effect estimate should be reflected in such situations. With that goal in mind, we construct a Gaussian process model for estimating treatment effects in the presence of practical violations of positivity. Advantages of our method include minimal distributional assumptions, a cohesive model for estimating treatment effects, and more uncertainty associated with areas in the covariate space where there is less overlap. We assess the performance of our approach with respect to bias and efficiency using simulation studies. The method is then applied to a study of critically ill female patients to examine the effect of undergoing right heart catheterization.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/yutatomimasu/Zotero/storage/S56QULFB/Zhu et al. - 2022 - Addressing Positivity Violations in Causal Effect Estimation using Gaussian Process Priors.pdf}
}
