---
title: "UTBoost: Gradient Boosted Decision Trees for Uplift Modeling"
Tags:
DOI: https://doi.org/10.1007/978-981-96-0116-5_4
cite key: gao2025
description:
rating:
---
## Projects

zotero: [Full Text PDF](zotero://select/library/items/8KWR6LC5)
# UTBoost: Gradient Boosted Decision Trees for Uplift Modeling

___
## ノート
### DDP(Delta-Delta-P algorithm)
このアルゴリズムの主な特徴と仕組みは以下の通りです。

### 1. アルゴリズムの目的

DDPアルゴリズムの主な目的は、分割によって生成される**左右の子ノード間における「アップリフト（因果効果）」の差を最大化すること**にあります。これにより、処置の効果が大きく異なるサブグループをデータから効率的に見つけ出すことができます。

### 2. 分割基準（スプリット基準）の仕組み

従来の決定木（CARTなど）では、ノード内のアウトカムの分散（MSE）や不純度（Gini不純度）を最小化するように分割を行いますが、DDPは**因果効果の異質性（Heterogeneity）**に焦点を当てます。

- **Delta-P（ΔP）:** 各ノード内における「処置群の平均結果」と「対照群の平均結果」の差（推定アップリフト）を指します。
- **Delta-Delta-P（ΔΔP）:** 分割後の左ノードのアップリフト（ΔP_L）と、右ノードのアップリフト（ΔP_R）の差を指します。

具体的には、以下の数値を最大化するような分割点を選択します： $$\frac{n_L n_R}{n} [(\bar{y}_{1L} - \bar{y}_{0L}) - (\bar{y}_{1R} - \bar{y}_{0R})]^2$$ ここで、$n_L, n_R$ は各ノードのサンプルサイズ、$(\bar{y}_{1} - \bar{y}_{0})$ は各ノード内の処置群と対照群の平均アウトカムの差を表します。

### 3. 先行研究と発展

- **起源:** このアプローチは、Hansotia and Rukstales (2002) による増分価値モデリング（Incremental Value Modeling）や、Su et al. (2009) によるサブグループ解析の研究に関連しています。
- **TDDPへの拡張:** ソースの一つである「UTBoost」論文では、このDDPの概念を勾配ブースティング（GBDT）に拡張した**TDDP（Transformed DDP）**が提案されています。TDDPは、各反復でサンプルラベルを変換し、前のモデルが学習した因果効果の残差を次段の木で逐次的に学習することで、高次元データにおいて従来の手法よりも高い精度を実現しています。

### まとめ

DDPアルゴリズムは、単に「誰が購入するか（アウトカム）」を予測するのではなく、分割によって**「誰に処置をすれば最も反応が変わるか（アップリフトの差）」を最大化するように木を成長させる**ための計算手法です。

### Causal TreeやCausal Forestとの違い
結論から申し上げますと、UTBoostはCausal TreeやCausal Forestと「同じ意思決定木をベースとした因果推論手法」という点では共通していますが、アンサンブル（学習の組み合わせ方）の仕組みが根本的に異なる**「別のアプローチ（進化形）」**です。

ソースに基づき、その違いと関係性を詳しく説明します。

### 1. アンサンブル手法の違い：Bagging vs. Boosting

最も大きな違いは、複数の木をどう組み合わせるかという点にあります。

- **Causal Forest:** **Bagging（バギング）**ベースの手法です。これは「ランダムフォレスト」の因果推論版であり、独立して構築した多数の因果木（Causal Trees）の予測値を平均化します。
- **UTBoost:** **Boosting（ブースティング）**ベースの手法です。具体的には**勾配ブースティング決定木（GBDT）**を因果推論に応用したものです。前のモデルの誤差（残差）を後続のモデルが逐次的に学習していくことで、精度を高めます。

### 2. Causal Forestに対するUTBoostの立ち位置

ソースでは、UTBoostは従来のバギングベースの手法（Causal Forestなど）が抱えていた課題を解決するための**「イノベーション（革新）」**として位置づけられています。

- **高次元データへの強さ:** Causal Forestなどのバギング手法は、共変量（特徴量）の数が増えると因果効果の予測能力が著しく低下する（性能の減衰）という問題がありました。UTBoostはブースティングを採用することで、高次元データにおいても複雑なパターンを効果的に捉え、より正確なアップリフト（増分影響）の推定を可能にしています。
- **学習の仕組み:** Causal Forestが「個々の因果木の平均」であるのに対し、UTBoost（特にその中の**TDDP**アルゴリズム）は、ラベルを変換しながら**因果効果を直接・逐次的に学習**していきます。

### 3. 具体的な手法（TDDPとCausalGBM）

UTBoostは、単一の木モデルではなく、以下の2つの新しいアルゴリズムを含むシステムです。

1. **TDDP (Transformed Delta-Delta-P):** 従来のアップリフトツリー（Causal Treeの類縁）で使われていた「左右の子ノード間のアップリフトの差を最大化する」という分割基準（DDP）を維持しつつ、これをブースティングの枠組みに拡張したものです。
2. **CausalGBM:** 「潜在的な対照群の結果」と「個別の因果効果」を単一の学習器で同時に最適化する、より高度な手法です。

### まとめ

UTBoostは、Causal TreeやCausal Forestの「親戚」ではありますが、**「バギングからブースティングへ」**とパラダイムをシフトさせた手法です。特に、実世界の産業界で扱われるような**「大規模かつ高次元なデータ」において、Causal Forestを凌駕する性能を出すこと**を目的に開発されました。

### バギングとブースティング
### 1. 学習プロセスの根本的な違い

- **バギング（Bootstrap Aggregating）:**
    
    - **独立した並列学習:** データセットからランダムにサンプリング（ブートストラップ法）して作成した複数のサブサンプルに対して、**複数のモデルを独立して構築**します。
    - **平均化による統合:** 最終的な予測値は、個々の独立したツリー（モデル）の予測値を**平均化**することで得られます。
    - **代表的な手法:** ランダムフォレスト、因果木（Causal Trees）、因果フォレスト（Causal Forests）、直交ランダムフォレスト（ORF）などがこれに該当します。
- **ブースティング（Boosting）:**
    
    - **逐次的な反復学習:** モデルを一つずつ順番に構築する**逐次的プロセス**をとります。
    - **誤差の修正:** 前の段階のモデルが捉えきれなかった**誤差（残差）を、後続のモデルが順次修正**するように学習を進めます。
    - **代表的な手法:** 勾配ブースティング決定木（GBDT）、UTBoost（TDDPやCausalGBMアルゴリズムを含む）などが代表例です。

### 2. 因果推論・アップリフトモデリングにおける特性の違い

ソースでは、特に高次元データにおける両者の性能差が強調されています。

- **高次元データへの対応力:**
    
    - **バギングベースの手法（因果フォレストなど）**は、共変量（特徴量）の数が増えると、**因果効果を予測する能力が著しく低下（性能が減衰）**するという課題が指摘されています。
    - 一方、**ブースティング（UTBoostなど）**は、先行するモデルが学習した因果効果の残差を次段で学習するため、**高次元データにおいても複雑なパターンを効果的に捉える**ことができ、バギングよりも大幅に優れた性能を発揮します。
- **学習の目的:**
    
    - バギングベースのアップリフトツリーは、分割によって「左右の子ノード間のアップリフトの差を最大化する」といった局所的な統計的乖離度に焦点を当てることが多いです。
    - ブースティング（特にCausalGBM）は、潜在的結果と因果効果の両方を一つの**多目的損失関数内で同時に最適化**し、グローバルな損失最小化を目指します。

### 3. メリットとデメリットの比較

|特徴|バギング（例：Causal Forest）|ブースティング（例：UTBoost）|
|:--|:--|:--|
|**学習順序**|並列（各モデルは独立）|逐次（前のモデルの誤差を修正）|
|**統合方法**|予測値の平均化|加法的なモデルの積み上げ|
|**高次元データ**|性能が低下しやすい|複雑なパターンに強く、精度が高い|
|**過学習**|比較的ロバスト|特徴量が多いと過学習のリスクがあり、正規化が必要になる場合がある|

**まとめると**、バギングは独立したモデルの平均化により安定性を図る手法であるのに対し、ブースティングはモデルを連鎖させて誤差を減らす手法です。特に産業界で扱われるような**大規模かつ高次元なデータでの因果推論においては、ブースティングがより高い予測能力を示す**傾向にあります。
___
## AI要約
___

date: 2024-12-03 categories: #アップリフトモデリング, #勾配ブースティング木 , #因果推論

### 1. どんなもの？

この論文は、特定の施策（処置）が個々の顧客に与える増分影響（アップリフト）を予測するための、勾配ブースティング決定木（GBDT）ベースの新しいシステム**UTBoost**を提案しています。具体的には、アップリフトを直接逐次学習する**TDDP（Transformed Delta-Delta-P）**と、潜在的結果と因果効果を単一の学習器で同時に学習する**CausalGBM**という2つの手法を導入しています。これにより、同一人物に対して処置群と対照群の両方の結果を同時に観察できないという「因果推論の根本問題（反事実のジレンマ）」に対応しています。

### 2. 先行研究と比べてどこがすごいの？

従来のアップリフトモデリングでは、ランダムフォレストに基づく手法（バギング）が一般的でしたが、共変量の数が増えると予測性能が著しく低下するという課題がありました。また、単一モデル（S-learner）は処置インジケーターが特徴量として選択されないリスクがあり、2モデル（T-learner）は誤差が累積しやすいという欠点がありました。UTBoostは**勾配ブースティング（ブースティング）**を採用することで、データの複雑なパターンを反復的に捉えることができ、特に**高次元データにおいてランダムフォレストよりも大幅に優れた性能**を発揮します。また、CausalGBMは単一のモデル内で因果効果とアウトカムの両方を最適化するため、情報の共有が可能になり、計算コストと誤差の累積を抑えることができます。

### 3. 技術や手法の"キモ"はどこにある？

- **TDDP:** 従来のDDPアルゴリズムを拡張し、各反復でサンプルラベルを変換（処置群のみ前段までの予測値を差し引く）することで、アップリフトの異質性を最大化するように決定木を逐次構築します。
- **CausalGBM:** 観測されたアウトカムを「潜在的な対照群の結果」と「個別の因果効果」の和として定式化し、GBDTのフレームワーク内でこれら2つを同時に学習します。
- **二次近似と効率化:** 2次勾配情報を利用して収束を早めるとともに、ランダム化比較試験（RCT）の条件下で計算量を大幅に削減する近似手法を導入しています。
- **分割基準の最適化:** アップリフトの平均二乗誤差（MSE）を最小化することが、左右の子ノード間の平均アップリフトの差を最大化することと等価であることを示し、実用的な分割基準として採用しています。

### 4. どうやって有効だと検証した？

3つの大規模な実世界データセット（CRITEO, HILLSTROM, VOUCHER）と1つの合成データセットを用いて検証を行いました。

- **評価指標:** 10分割交差検証を行い、正規化されたQini係数で評価しました。
- **結果:** CausalGBMは、既存のメタラーナー（X-learner, R-learnerなど）やアップリフトランダムフォレストといった**全てのベースライン手法を上回る性能**を一貫して示しました。特に特徴量が多いデータセットにおいて、バギングベースの手法よりも大幅に高いQiniスコアを達成しました。

### 5. 議論はあるか？

- **過学習のリスク:** 局所的な因果効果の異質性に焦点を当てるTDDPは、特徴量次元が増えると過学習しやすい傾向があり、グローバルな損失関数を最適化するCausalGBMの方が汎化性能において堅牢であることが示唆されました。
- **次元の影響:** 低次元の設定ではブースティングとバギングの差は僅かですが、高次元になるほどブースティングの優位性が顕著になります。

### 6. 次に読むべき論文はあるか？

- 決定木の再帰的分割の基礎として、**Athey and Imbens (2016)** "Recursive partitioning for heterogeneous causal effects"。
- CATE推定の準オラクル性能については、**Nie and Wager (2021)** "Quasi-oracle estimation of heterogeneous treatment effects"。
- 一般化ランダムフォレストについては、**Athey et al. (2019)** "Generalized random forests"。

### 論文情報・リンク

- [Junjie Gao, et al., "UTBoost: Gradient Boosted Decision Trees for Uplift Modeling," PRICAI 2024: Trends in Artificial Intelligence, vol. 15281, 2024.](https://arxiv.org/pdf/2312.02573)

## 注釈


%% Import Date: 2026-01-26T09:21:14.661+09:00 %%
