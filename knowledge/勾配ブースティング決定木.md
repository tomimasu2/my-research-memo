
## 決定木
- CART(Classification And Regression Tree)
	- 二分木構造
	- 分割基準：各ノード内でできるだけ近い値やクラスを持つサンプルで構成されるように、最も整った（=不純度が小さくなくるような）分割条件を選択する
		- 不純度：不純度が低いほど、特定のクラスに偏っている
			- 分類：ジニ不純度、エントロピー
				- ジニ不純度：あるノードにどれだけ複数のクラスが混在しているか
			- 回帰：平均二乗誤差、平均絶対誤差
		- 情報利得（Information Gain）:
			- 分割によってどれだけ情報を得られたか
			- 分割前のノードの不純度-分割後のすべてのノードの不純度の和
			- 情報利得が大きいと、データをより良く分割できている
	- 剪定（Pruning）：過学習を抑制
		- CARTでは、コスト複雑度剪定（cost-complexity Pruning)が一般的に用いられる
			- 決定木の誤差と葉の数の和を最小化する
			- 正則化パラメータで複雑さを制御

## アンサンブル学習
- バギング（Bagging）：（例：ランダムフォレスト）
	- 同じモデルを並列に学習させて、その予測値を平均、または多数決で統合する手法
	- 各モデルの学習データは、データセットからブートストラップ（復元抽出）によって作成されるため、モデルの過学習の抑制と精度安定が期待される
	- ランダムフォレストでは、分岐に使用する特徴量をランダムに選択
- ブースティング（Boosting）：
	- 弱学習器を直列に学習させて、段階的に誤差を小さくするようにモデルを構築する
	- 勾配ブースティング
		- 損失関数の勾配を用いて、残差（＝勾配）を削減していく手法
- スタッキング（Stacking）
	- 複数の種類のモデルの予測結果を組み合わせて、それを入力としたメタモデルを構築する
## 勾配降下法
- ステージワイズ最適化：1つずつ弱学習器を追加して最適化する

## 主要モデル
 
### XGBoost / LightGBM / CatBoost  


---

## 1. 全体像（思想の違い）

| 観点 | XGBoost | LightGBM | CatBoost |
|---|---|---|---|
| 主眼 | 汎用・安定・理論的に整理 | 大規模高速学習 | カテゴリ変数とリーク対策 |
| 木構築 | Level-wise | **Leaf-wise** | Level-wise |
| 特徴 | 正則化 + 2次近似 | ヒストグラム + leaf拡張 | Ordered Boosting |

---

## 2. 損失最適化の共通点と差分

### 共通点
- すべて **2次のテイラー近似** に基づく勾配ブースティング
- 各サンプルで  
  - 1次微分 \(g_i\)  
  - 2次微分 \(h_i\)  
  を用いる

$$
\mathcal{L} \approx \sum_i \left( g_i f(x_i) + \frac{1}{2} h_i f(x_i)^2 \right)
$$

### 差分（使い方）
| 項目      | XGBoost | LightGBM | CatBoost          |
| ------- | ------- | -------- | ----------------- |
| 勾配の扱い   | 標準      | 標準       | **順序付き（Ordered）** |
| Hessian | 常に使用    | 常に使用     | 使用                |

---

## 3. 木構築アルゴリズムの違い

### 3.1 XGBoost：Level-wise（レベル成長）

- 深さを1段ずつ均等に拡張
- 木の形が安定
- 過学習しにくい

**メリット**
- 理論的に扱いやすい
- 中小規模データで安定

**デメリット**
- 不要な分割も行われがち
- 大規模データでは遅め

---

### 3.2 LightGBM：Leaf-wise（リーフ優先成長）

- **損失減少が最大の葉だけを分割**
- 非常に深い木ができやすい

**メリット**
- 学習速度が速い
- 少ない木で高精度

**デメリット**
- 過学習しやすい  
→ `max_depth`, `num_leaves` が必須

---

### 3.3 CatBoost：Level-wise + Ordered設計

- 木構造自体は Level-wise
- 重点は **学習データの使い方**

---

## 4. カテゴリ変数処理

| 観点 | XGBoost | LightGBM | CatBoost |
|---|---|---|---|
| 基本 | One-hot / Encoding | One-hot / Encoding | **内蔵処理** |
| 高基数 | 弱い | やや弱い | **強い** |
| リーク対策 | なし | なし | **あり** |

### Ordered Target Encoding

- 各サンプルより**前のデータのみ**で統計量を計算
- Target leakage を理論的に抑制

---

## 5. Ordered Boosting（CatBoost独自）

### 問題意識
通常のGBDTでは：
- 同一データで
  - 勾配計算
  - 木学習
- → **ターゲットリーク**

### CatBoostの解決策
- データに順序を付与
- 各時点で「過去データのみ」を使用


---

## 6. ヒストグラム最適化

| 項目 | XGBoost | LightGBM | CatBoost |
|---|---|---|---|
| Bin化 | optional | **標準** | 標準 |
| メモリ効率 | 中 | **高** | 中 |
| GPU適性 | 高 | 高 | 高 |

---

## 7. 正則化と安定性

| 観点 | XGBoost | LightGBM | CatBoost |
|---|---|---|---|
| L1/L2 | 明示的 | 明示的 | 暗黙的 |
| 過学習耐性 | 高 | 低〜中 | **高** |
| ノイズ耐性 | 高 | やや低 | 高 |

---

